{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d4e1a7-5550-4200-8632-0be919481609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 경고 뜨지 않게 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 그래프 설정\n",
    "sns.set()\n",
    "\n",
    "# 그래프 기본 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['figure.figsize'] = 12, 6\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 데이터 전처리 알고리즘\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 학습용과 검증용으로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 교차 검증\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 평가함수\n",
    "# 분류용\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 회귀용\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 모델의 최적의 하이퍼 파라미터를 찾기 위한 도구\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# 학습 모델 저장을 위한 라이브러리\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7211b-feab-4bcf-9739-888c2897ad0a",
   "metadata": {},
   "source": [
    "### 프로젝트 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c94f59-cd5f-4895-99e7-4880c294faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료된 모델을 저장할 파일 이름\n",
    "best_model_path = 'model/best_model_regression_fa_2016.dat'\n",
    "# 교차검증 횟수\n",
    "cv_count = 10\n",
    "# 교차 검증\n",
    "kfold = KFold(n_splits=cv_count, shuffle=True, random_state=1)\n",
    "# 평가 결과를 담을 리스트\n",
    "# 필요하다면 다른 것도 만들어주세요\n",
    "mse_score_list = []\n",
    "# 학습 모델 이름\n",
    "model_name_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5f627-a061-489c-90d2-b09a7dd4df89",
   "metadata": {},
   "source": [
    "### 데이터 준비\n",
    "- 데이터를 읽어오고 필요한 전처리까지 다 한다음 입력데이터는 train_X, 결과데이터는 train_y라는 변수에 담아서 준비해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffe7938-cef9-42fd-966a-0b733ed02e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 불러온다.\n",
    "df1 = pd.read_csv('머신러닝_2013~2016FA.csv')\n",
    "# 입력과 결과로 나눈다.\n",
    "X = df1.drop('1년당 계약 총액', axis=1)\n",
    "y = df1['1년당 계약 총액']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7e8287-40cb-43b5-bc22-14c7abcce0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(X)\n",
    "X2 = scaler1.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7266d6-0381-4b19-b4a6-cf72fb10088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습할 데이터를 변수에 담아준다.\n",
    "train_X = X2\n",
    "train_y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ee66e-20c3-4c64-9547-8cf28cdd9b25",
   "metadata": {},
   "source": [
    "### 기본 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebeb36bc-093d-4035-b12f-21b7b3dd0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn_basic_model = KNeighborsRegressor()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(knn_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"KNN Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cefe24fa-7d7f-4961-ae0b-0bd30f3b59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "lr_basic_model = LinearRegression()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(lr_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"LinearRegression Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6719db0e-b866-4010-b6a6-d8af9543786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "ridge_basic_model = Ridge()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(ridge_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"Ridge Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac97ba9-95b0-4947-a187-55062f526409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "lasso_basic_model = Lasso()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(lasso_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"Lasso Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "825bc39d-4ec5-49cb-b5b2-44cea95d8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "en_basic_model = ElasticNet()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(en_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"ElasticNet Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7744eb3-8063-4410-8bfc-dd04a8760df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "svr_basic_model = SVR()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(svr_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"SVR Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c65f4f4-1985-417a-a1bc-b766e0f5ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree\n",
    "tree_basic_model = DecisionTreeRegressor()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(tree_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"DecisionTree Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa46ec8-4ba1-4f7e-bbb1-c99cfeafbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rf_basic_model = RandomForestRegressor()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(rf_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"RandomForest Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9ad664-4d86-497e-a162-ab1510ac6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "ada_basic_model = AdaBoostRegressor()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(ada_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"AdaBoost Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59c7a17f-67e6-41db-9105-84f486c3add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoost\n",
    "gb_basic_model = GradientBoostingRegressor()\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(gb_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"GradientBoost Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b2bcf05-9f40-4864-b161-bbcc6af6baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM\n",
    "lgbm_basic_model = LGBMRegressor(verbose=-1)\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(lgbm_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"LGBM Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed5b9cab-c5ee-47c8-8eb3-6eacab91d6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "xgb_basic_model = XGBRegressor(verbose=-1, slient=True)\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(xgb_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"XGBoost Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4067d4d-7ccc-4c50-b754-9962b2c117c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n",
      "[18:16:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"slient\", \"verbose\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Voting을 구성하기 위한 모델 목록\n",
    "model1 = KNeighborsRegressor()\n",
    "model2 = LinearRegression()\n",
    "model3 = Ridge()\n",
    "model4 = Lasso()\n",
    "model5 = ElasticNet()\n",
    "model6 = SVR()\n",
    "model7 = DecisionTreeRegressor()\n",
    "model8 = RandomForestRegressor()\n",
    "model9 = AdaBoostRegressor()\n",
    "model10 = GradientBoostingRegressor()\n",
    "model11 = LGBMRegressor(verbose=-1)\n",
    "model12 = XGBRegressor(verbose=-1, slient=True)\n",
    "\n",
    "hard_voting_basic_model_list = [\n",
    "    ('model1', model1),\n",
    "    ('model2', model2),\n",
    "    ('model3', model3),\n",
    "    ('model4', model4),\n",
    "    ('model5', model5),\n",
    "    ('model6', model6),\n",
    "    ('model7', model7),\n",
    "    ('model8', model8),\n",
    "    ('model9', model9),\n",
    "    ('model10', model10),\n",
    "    ('model11', model11),\n",
    "    ('model12', model12),\n",
    "]\n",
    "\n",
    "hard_voting_basic_model = VotingRegressor(estimators=hard_voting_basic_model_list)\n",
    "\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(hard_voting_basic_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"HardVoting Basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "230b74db-797d-4030-ad28-17036423f54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN Basic</th>\n",
       "      <td>8.789273e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression Basic</th>\n",
       "      <td>9.213659e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Basic</th>\n",
       "      <td>8.896209e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Basic</th>\n",
       "      <td>9.466509e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet Basic</th>\n",
       "      <td>9.296319e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR Basic</th>\n",
       "      <td>4.606492e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree Basic</th>\n",
       "      <td>1.298146e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest Basic</th>\n",
       "      <td>8.719160e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost Basic</th>\n",
       "      <td>7.816161e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoost Basic</th>\n",
       "      <td>6.355524e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM Basic</th>\n",
       "      <td>9.305125e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost Basic</th>\n",
       "      <td>7.531284e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HardVoting Basic</th>\n",
       "      <td>6.304783e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 mse\n",
       "KNN Basic               8.789273e+08\n",
       "LinearRegression Basic  9.213659e+08\n",
       "Ridge Basic             8.896209e+08\n",
       "Lasso Basic             9.466509e+08\n",
       "ElasticNet Basic        9.296319e+08\n",
       "SVR Basic               4.606492e+09\n",
       "DecisionTree Basic      1.298146e+09\n",
       "RandomForest Basic      8.719160e+08\n",
       "AdaBoost Basic          7.816161e+08\n",
       "GradientBoost Basic     6.355524e+08\n",
       "LGBM Basic              9.305125e+08\n",
       "XGBoost Basic           7.531284e+08\n",
       "HardVoting Basic        6.304783e+08"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = {\n",
    "    'mse' : mse_score_list\n",
    "}\n",
    "test_df = pd.DataFrame(d1, index=model_name_list)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95a4d6-00f5-473a-aef2-420beb692bc1",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1933ba33-0263-463e-8c2b-5a5f2d79e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'n_neighbors' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "knn_tuning_model = KNeighborsRegressor()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "knn_grid_clf = GridSearchCV(knn_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "knn_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(knn_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"KNN Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1eb6b2e-499b-4edb-a94d-1a3e5f320b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "ridge_tuning_model = Ridge()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "ridge_grid_clf = GridSearchCV(ridge_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "ridge_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(ridge_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"Ridge Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b4c2f08-625e-4607-aaba-9dc38bd91f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "lasso_tuning_model = Lasso()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "lasso_grid_clf = GridSearchCV(lasso_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "lasso_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(lasso_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"Lasso Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51411fd3-4cb2-4bba-ab20-ca452c321fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "en_tuning_model = ElasticNet()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "en_grid_clf = GridSearchCV(en_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "en_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(en_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"ElasticNet Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "771331d9-1f56-4636-bde2-1dbd83e82a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'C' : [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "svr_tuning_model = SVR()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "svr_grid_clf = GridSearchCV(svr_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "svr_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(svr_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"SVR Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a97dacb-42cb-42e7-81bb-08ee17fc7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'max_depth' : [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "tree_tuning_model = DecisionTreeRegressor()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "tree_grid_clf = GridSearchCV(tree_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "tree_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(tree_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"DecisionTree Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8582b19a-90c2-4dad-85d5-0d8049f81438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 최적의 하이퍼 파라미터를 찾는다.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m rf_grid_clf \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_tuning_model, param_grid\u001b[38;5;241m=\u001b[39mparams, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39mkfold)\n\u001b[1;32m----> 9\u001b[0m rf_grid_clf\u001b[38;5;241m.\u001b[39mfit(train_X, train_y)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 평가 결과를 담아준다.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m mse_score_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mabs\u001b[39m(rf_grid_clf\u001b[38;5;241m.\u001b[39mbest_score_))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'n_estimators' : [50, 100, 150, 200, 250, 300]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "rf_tuning_model = RandomForestRegressor()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "rf_grid_clf = GridSearchCV(rf_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "rf_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(rf_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"RandomForest Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8afc5-369e-4af9-84fc-0d3779fa7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'n_estimators' : [50, 100, 150, 200, 250, 300],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1, 0, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "ada_tuning_model = AdaBoostRegressor()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "ada_grid_clf = GridSearchCV(ada_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "ada_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(ada_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"AdaBoost Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44391d00-8fa7-416b-992a-1d89091d8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'n_estimators' : [50, 100, 150, 200, 250, 300],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1, 0, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "gb_tuning_model = GradientBoostingRegressor()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "gb_grid_clf = GridSearchCV(gb_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "gb_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(gb_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"GradientBoost Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa5f24-360b-4245-9f51-48916ceb1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'n_estimators' : [50, 100, 150, 200, 250, 300],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1, 0, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "lgbm_tuning_model = LGBMRegressor()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "lgbm_grid_clf = GridSearchCV(lgbm_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "lgbm_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(lgbm_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"LGBM Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34909f55-8935-466f-8e43-36fb71e009ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝할 하이퍼 파라미터 후보 값\n",
    "params = {\n",
    "    'booster' : ['gbtree', 'gblinear'],\n",
    "    'n_estimators' : [50, 100, 150, 200, 250, 300],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1, 0, 10, 100, 1000, 10000]\n",
    "}\n",
    "# 사용할 모델 객체를 생성한다.\n",
    "xgb_tuning_model = XGBRegressor()\n",
    "# 최적의 하이퍼 파라미터를 찾는다.\n",
    "xgb_grid_clf = GridSearchCV(xgb_tuning_model, param_grid=params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "xgb_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(xgb_grid_clf.best_score_))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"XGB Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e11a2f-6888-4f19-b994-f8013ab4ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting을 구성하기 위한 모델 목록\n",
    "model1 = KNeighborsRegressor()\n",
    "model2 = LinearRegression()\n",
    "model3 = Ridge()\n",
    "model4 = Lasso()\n",
    "model5 = ElasticNet()\n",
    "model6 = SVR()\n",
    "model7 = DecisionTreeRegressor()\n",
    "model8 = RandomForestRegressor()\n",
    "model9 = AdaBoostRegressor()\n",
    "model10 = GradientBoostingRegressor()\n",
    "model11 = LGBMRegressor(verbose=-1)\n",
    "model12 = XGBRegressor(verbose=-1, slient=True)\n",
    "\n",
    "# 튜닝 과정에서 찾은 최적의 하이퍼 파라미터를 셋팅해준다.\n",
    "model1.set_params(**knn_grid_clf.best_params_)\n",
    "model3.set_params(**ridge_grid_clf.best_params_)\n",
    "model4.set_params(**lasso_grid_clf.best_params_)\n",
    "model5.set_params(**en_grid_clf.best_params_)\n",
    "model6.set_params(**svr_grid_clf.best_params_)\n",
    "model7.set_params(**tree_grid_clf.best_params_)\n",
    "model8.set_params(**rf_grid_clf.best_params_)\n",
    "model9.set_params(**ada_grid_clf.best_params_)\n",
    "model10.set_params(**gb_grid_clf.best_params_)\n",
    "model11.set_params(**lgbm_grid_clf.best_params_)\n",
    "model12.set_params(**xgb_grid_clf.best_params_)\n",
    "\n",
    "\n",
    "hard_voting_tuning_model_list = [\n",
    "    ('model1', model1),\n",
    "    ('model2', model2),\n",
    "    ('model3', model3),\n",
    "    ('model4', model4),\n",
    "    ('model5', model5),\n",
    "    ('model6', model6),\n",
    "    ('model7', model7),\n",
    "    ('model8', model8),\n",
    "    ('model9', model9),\n",
    "    ('model10', model10),\n",
    "    ('model11', model11),\n",
    "    ('model12', model12),\n",
    "]\n",
    "\n",
    "hard_voting_tuning_model = VotingRegressor(estimators=hard_voting_tuning_model_list)\n",
    "\n",
    "# 교차 검증을 수행한다\n",
    "r1 = cross_val_score(hard_voting_tuning_model, train_X, train_y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "# 평가 결과를 담아준다.\n",
    "mse_score_list.append(abs(r1.mean()))\n",
    "# 학습 모델 이름을 담아준다.\n",
    "model_name_list.append(\"HardVoting Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e01e3-61c2-47c5-bb2a-97ea4ac2e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {\n",
    "    'mse' : mse_score_list\n",
    "}\n",
    "result_df = pd.DataFrame(d1, index=model_name_list)\n",
    "result_df.sort_values(by='mse', inplace=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7c81b-79e1-4ffa-a9e8-f79ef99ae198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 예측한다.\n",
    "hard_voting_tuning_model.fit(train_X, train_y)\n",
    "\n",
    "y_pred = hard_voting_tuning_model.predict(train_X)\n",
    "\n",
    "plt.plot(train_y.values, label='원본결과')\n",
    "plt.plot(y_pred, label='예측결과')\n",
    "plt.title('HardVoting Tuning')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2b6f3-b975-4391-a501-8e19882228bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "y_pred = knn_grid_clf.best_estimator_.predict(train_X)\n",
    "\n",
    "plt.plot(train_y.values, label='원본결과')\n",
    "plt.plot(y_pred, label='예측결과')\n",
    "plt.title('KNN Tuning')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84a272-c301-494c-bb82-63c9f7b20435",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_basic_model.fit(train_X, train_y)\n",
    "\n",
    "y_pred = gb_basic_model.predict(train_X)\n",
    "\n",
    "plt.plot(train_y.values, label='원본결과')\n",
    "plt.plot(y_pred, label='예측결과')\n",
    "plt.title('GradientBoost Basic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a313960-a6cf-47c1-97d0-4f8b73822fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_grid_clf.fit(train_X, train_y)\n",
    "\n",
    "y_pred = gb_grid_clf.predict(train_X)\n",
    "\n",
    "plt.plot(train_y.values, label='원본결과')\n",
    "plt.plot(y_pred, label='예측결과')\n",
    "plt.title('GradientBoost Tuning')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7397f-fc49-487f-b9ec-8271016d72f6",
   "metadata": {},
   "source": [
    "### 학습 모델 등을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159e2fb-bc64-4684-b80b-aa8d316811c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "10/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbc94e-5539-4f0c-82ff-5423a806c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 객체를 생성한다.\n",
    "best_model = GradientBoostingRegressor()\n",
    "# 하이퍼 파라미터를 설정한다.\n",
    "# best_model.set_params(**gb_grid_clf.best_params_)\n",
    "# 전체 데이터를 학습시킨다.\n",
    "best_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fbb6a-3de9-4930-99dd-66e4fea5154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 등을 저장한다.\n",
    "with open(best_model_path, 'wb') as fp :\n",
    "    pickle.dump(best_model, fp)\n",
    "    pickle.dump(scaler1, fp)\n",
    "\n",
    "print('저장완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4a97c-8164-4fa9-ad41-c2ed8ac2a18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29387796-470c-459f-93d9-4914ccdb3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KNeighborsRegressor()  # Basic\n",
    "# model = knn_grid_clf.best_estimator_  # Tuned (GridSearchCV 결과)\n",
    "\n",
    "# model = LinearRegression()\n",
    "\n",
    "# model = Ridge()  # Basic\n",
    "# model = ridge_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = Lasso()  # Basic\n",
    "# model = lasso_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = ElasticNet()  # Basic\n",
    "# model = en_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = SVR()  # Basic\n",
    "# model = svr_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = DecisionTreeRegressor()  # Basic\n",
    "# model = tree_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = RandomForestRegressor()  # Basic\n",
    "# model = rf_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = AdaBoostRegressor()  # Basic\n",
    "# model = ada_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = GradientBoostingRegressor()  # Basic\n",
    "model = gb_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = LGBMRegressor(verbose=-1)  # Basic\n",
    "# model = lgbm_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = XGBRegressor(verbosity=0)  # Basic\n",
    "# model = xgb_grid_clf.best_estimator_  # Tuned\n",
    "\n",
    "# model = VotingRegressor(estimators=hard_voting_basic_model_list)  # Basic\n",
    "# model = VotingRegressor(estimators=hard_voting_tuning_model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad1314-a531-41d3-a361-07d71b0b42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(train_X)\n",
    "\n",
    "# 평가 지표 계산\n",
    "mse = mean_squared_error(train_y, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(train_y, y_pred)\n",
    "r2 = r2_score(train_y, y_pred)\n",
    "\n",
    "# 출력\n",
    "print(f\"모델: {model.__class__.__name__}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5d442-0bc9-4d3f-995b-75a3596b5472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
